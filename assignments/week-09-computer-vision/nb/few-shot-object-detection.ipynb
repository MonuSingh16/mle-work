{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhYWFsyXDk53"
      },
      "source": [
        "<p align = \"center\" draggable=â€falseâ€ ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
        "     width=\"200px\"\n",
        "     height=\"auto\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uasae8v2jR6Q"
      },
      "source": [
        "# <h1 align=\"center\" id=\"heading\">Object Detection</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSlxEZS0PboT"
      },
      "source": [
        "Today you are a machine learning engineer on the Spatial Perception Team (SPT) at Apple. The goal is to levarage an existing object detector model to automatically detect dogs, an application of **transfer learning**. \n",
        "\n",
        "The idea is that you have access to a model that you or one of your colleagues has already trained on a large and diverse set of images, but the model is not very specific for your task (dog detection). We'll do transfer learning by fine-tuning this existing model on a small dataset of dog images. \n",
        "\n",
        "*This is only a small part of the end product -- Visual Look Up feature released in iOS 15 -- snap a picture, identify if an object belonging to five categories (art, landmarks, nature, books, and pets) exists, if yes, highlight with a object symbol, and further identify, e.g., the species of a plant. Examples are shown below ([picture credits](https://www.iphonetricks.org/how-to-visual-look-up-photos-on-iphone/))*\n",
        "\n",
        "<div>\n",
        "<img src=\"https://149493502.v2.pressablecdn.com/wp-content/uploads/2021/07/visual-look-up-categories.jpg\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJZKvl8iPboT"
      },
      "source": [
        "## ðŸ“š Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAdsBLsJPboU"
      },
      "source": [
        "At the end of this session, you will be able to:\n",
        "\n",
        "- understand [few-shot learning (FSL)](https://neptune.ai/blog/understanding-few-shot-learning-in-computer-vision)\n",
        "- understand the mechanics of using pretrained model and fine tune a pretrained model \n",
        "- calculate metrics and evaluate detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyEDnQ0Mq9v2"
      },
      "source": [
        "## Task 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYlCWY1PEQn5"
      },
      "source": [
        "1. We will run this notebook via Google Colab to take advantage of its free GPU computing power and avoid installation pain.\n",
        "  \n",
        "    It is however not hassle-free, these three steps will ensure that you will not have any errors running this notebook (this should look familiar to you if you have run the demo notebook):\n",
        "    - step 1. `Runtime` > `Disconnect and delete runtime`\n",
        "    - step 2. make sure the runtime is **GPU**: `Runtime` > `Change runtime type`\n",
        "    - step 3. run the following cell to install `tensorflow` 2.8, as well as the compatible GPU-accelerated library; solution is based on this [issue](https://github.com/tensorflow/models/issues/10558). Depending on the internet connection, this may take a few minutes to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mTOcJsYB5Nz",
        "outputId": "6c7da079-90cc-46f1-ba67-55113fea2040"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tensorflow==2.8\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C8v6FHvPboV"
      },
      "source": [
        "Use the NVIDIA System Management Interface to check the GPU device you are on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFVLLExKyXP6",
        "outputId": "de73de05-5b44-4b8b-fed5-28d2394453c2"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jPOWXsyHoYq"
      },
      "source": [
        "2. Mount the drive to ensure access to where you store the images on Google Drive. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CekFQtNTm-48",
        "outputId": "37fc9d08-07f1-492e-d38f-439993388d5a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWcoAgCfq9v4"
      },
      "source": [
        "3. Clone the [tensorflow models repository](https://github.com/tensorflow/models) so we can make use of a pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUp8QOmv94ji",
        "outputId": "44cd498b-0fa3-41cf-995d-f3eb10e7338d"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "    while \"models\" in pathlib.Path.cwd().parts:\n",
        "        os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "    !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN0HwfO7LDsl"
      },
      "source": [
        "4. Install dependencies of the pre-trained models and the TF Object Detection API. This may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Qftm6W9Ffz",
        "outputId": "68f7d980-3771-4920-cee7-18374250c3a3"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install -q -U ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55tkny4FjR6V"
      },
      "source": [
        "## Task 2: Load and examine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUR801vcPboX"
      },
      "source": [
        "1. Make sure to upload the folder `dog_dataset` containing all image and annotation files to your google drive `/content/drive/My Drive/fourthbrain/dog_dataset`. \n",
        "\n",
        "    Retrieve a list containing the names of the training images in the directory given by `train_image_dir`. Use `os.listdir` and store the results to `image_names`; e.g., \n",
        "    ```python\n",
        "    image_names[0] # dog_004.jpg\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF7D0L-sq9v6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dog_dataset_path = '/content/drive/My Drive/fourthbrain/dog_dataset' # Replace with actual path\n",
        "train_image_dir = os.path.join(dog_dataset_path, 'train/images')\n",
        "image_names = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LugEHLMvIDk-"
      },
      "outputs": [],
      "source": [
        "assert len(image_names) == 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3cijw0vjR6Y"
      },
      "source": [
        "2. The function `load_image_into_numpy_array()` below is provided to load the an image given a `path`.\n",
        "\n",
        "    Examine the code and use it to read all the training images and store them in a list `train_images_np`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          5
        ],
        "id": "N0ylBRNrMhvP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from six import BytesIO\n",
        "import numpy as np\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"\n",
        "    Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note the convention that we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "    path: a file path.\n",
        "\n",
        "    Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "    im_width, im_height = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0yiH_iOjR6a"
      },
      "outputs": [],
      "source": [
        "train_images_np = []\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIorlKAzIbOE"
      },
      "outputs": [],
      "source": [
        "assert len(train_images_np) == 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6iGc-dcPboY"
      },
      "outputs": [],
      "source": [
        "assert train_images_np[0].shape == (480, 640, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RLwTrHDq9v8"
      },
      "source": [
        "3. What is the shape of each image (width, height and number of color channels)? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM_7PpeWjR6a",
        "outputId": "8e3b747f-c788-443b-8229-2c61867a0f30"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x67RkhApq9v8"
      },
      "source": [
        "4. Use the following code to draw all 10 images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RfW0LUTcq9v8",
        "outputId": "a24a4028-da1e-40ea-a50f-a7f12d810be4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "    ax = plt.subplot(2, len(train_images_np) / 2, idx + 1)\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    plt.imshow(train_image_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mSC6_SJMqKr"
      },
      "source": [
        "5. Recall our task is to detect where the dog is in the image. \n",
        "\n",
        "    The object detection model we'll use places bounding boxes around where it thinks there **might** be an object. For each bounding box, it also makes a prediction of which class the object is from, and there is an associated \"score\", or confidence, of this prediction. \n",
        "\n",
        "    The format for specifying bounding boxes of our training data and the model outputs are [$y_{\\min}$, $x_{\\min}$, $y_{\\max}$, $x_{\\max}$], where $y$ is the vertical position and $x$ is the horizontal position. For example, a bounding box of [0.1, 0.15, 0.8, 0.9] means one whose lower left corner is at position (0.1, 0.15) and whose upper right corner is at (0.8, 0.9).\n",
        "\n",
        "    These values, ranging between 0 and 1, are **agnostic** to the size of the image, if you wanted to convert them to pixels you would multiply by the width or height of the image (in pixels). \n",
        "\n",
        "    We will use the function `visualize_boxes_and_labels_on_image_array` in the object detection visualization utilities to display the bounding boxes overlaid on an image.\n",
        "\n",
        "    First, let's read the annotations (including the ground-truth bounding boxes) for these training images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWL2hsY_PboZ"
      },
      "source": [
        "6. Use function `read_content()` below to read in all the annotations and save them in a list `gt_boxes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "id": "PTGQsbpTjR6b"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from typing import Tuple\n",
        "\n",
        "def read_content(xml_file: str, h: int, w: int) -> Tuple[str, list]:\n",
        "    \"\"\" parse metadata/annotation \n",
        "    \n",
        "    Args:\n",
        "        xml_file (str): path to a xml file\n",
        "        h (int): height\n",
        "        w (int): width\n",
        "        \n",
        "    Return:\n",
        "        Tuple[str, list]: \n",
        "    \"\"\"\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    list_with_all_boxes = []\n",
        "\n",
        "    for boxes in root.iter('object'):\n",
        "        filename = root.find('filename').text\n",
        "\n",
        "        ymin, xmin, ymax, xmax = None, None, None, None\n",
        "        ymin = int(boxes.find(\"bndbox/ymin\").text) / h\n",
        "        ymax = int(boxes.find(\"bndbox/ymax\").text) / h\n",
        "        xmin = int(boxes.find(\"bndbox/xmin\").text) / w\n",
        "        xmax = int(boxes.find(\"bndbox/xmax\").text) / w\n",
        "\n",
        "        list_with_single_boxes = [ymin, xmin, ymax, xmax]\n",
        "        list_with_all_boxes.append(list_with_single_boxes)\n",
        "\n",
        "    return filename, list_with_all_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io8CI0Poq9v-"
      },
      "outputs": [],
      "source": [
        "train_ann_dir = os.path.join(dog_dataset_path, 'train/annotations')\n",
        "gt_boxes = []\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmNZUQCqPboa"
      },
      "outputs": [],
      "source": [
        "assert len(gt_boxes) == len(train_images_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM_j2_zhsH8U"
      },
      "source": [
        "7. Inspect and run the following code to create the target tensors for our model to use in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZLLSEyqjR6b"
      },
      "outputs": [],
      "source": [
        "dog_class_id = 1\n",
        "num_classes = 1\n",
        "\n",
        "category_index = {dog_class_id: {'id': dog_class_id, 'name': 'dog'}}\n",
        "\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for train_image_np, gt_box_np in zip(train_images_np, gt_boxes):\n",
        "    train_image_tensors.append(\n",
        "        tf.expand_dims(\n",
        "            tf.convert_to_tensor(train_image_np, dtype=tf.float32),\n",
        "            axis=0))\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "    gt_classes_one_hot_tensors.append(\n",
        "        tf.one_hot(zero_indexed_groundtruth_classes, num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrKbRTbUWlD"
      },
      "source": [
        "8. Call the function `visualize_boxes_and_labels_on_image_array()` to plot these bounding boxes on their respective images. Look at the images which show the dogs correctly identified in each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "7iHGAeBdjR6c",
        "outputId": "29dc9ab9-28cd-4be1-cc32-443bd2b2f982"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "dummy_scores = np.array([1.0], dtype=np.float32)\n",
        "plt.figure(figsize=(20, 3))\n",
        "for idx in range(5):\n",
        "    ax = plt.subplot(1, 5, idx + 1)\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    temp_img_copy = np.copy(train_images_np[idx])\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        temp_img_copy,\n",
        "        gt_boxes[idx],\n",
        "        np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "        dummy_scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        min_score_thresh=0.0)\n",
        "    plt.imshow(temp_img_copy)\n",
        "    print(f'Bounding box for image {idx}: ', gt_boxes[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKvtKtD2jR6c"
      },
      "source": [
        "9. Follow the similar process to load the test images and save them into the list `test_images_np`, except that each image is expected in `NHWC` format with `N` is set to be 1. Hint: use `np.expand_dims`. \n",
        "\n",
        "  Check [Table 1. Parameters defining a convolution](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#convo-intro)  for what each letter means in format such as `NHWC`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edYWiehqjR6c",
        "outputId": "abb7bc8c-0ed7-49b8-a0aa-e1a91327571b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "test_images_np = []\n",
        "test_image_dir = os.path.join(dog_dataset_path, 'test/images')\n",
        "t_image_names = os.listdir(test_image_dir)\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq0MXObHPbob"
      },
      "outputs": [],
      "source": [
        "assert len(test_images_np) == 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mFh13xFQAjv"
      },
      "outputs": [],
      "source": [
        "assert test_images_np[0].shape == (1, 480, 640, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJwoCg82q9wB"
      },
      "source": [
        "10. Follow the same process as above to load in the bounding box annotations for the test images. Save them in `t_gt_boxes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gWB8xwZjR6d"
      },
      "outputs": [],
      "source": [
        "t_gt_boxes = []\n",
        "test_ann_dir = os.path.join(dog_dataset_path, 'test/annotations/xmls')\n",
        "t_image_names = os.listdir(test_ann_dir)\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAw1R7-wq9wB"
      },
      "outputs": [],
      "source": [
        "assert len(test_images_np) == len(t_gt_boxes) == 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4mKdVo-jR6d"
      },
      "source": [
        "## Task 3: Load and run object detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L9AwwgOq9wC"
      },
      "source": [
        "1. Download the checkpoint and put it into `models/research/object_detection/test_data/`.\n",
        "\n",
        "    This model is an [SSD (single shot multibox detector)](https://arxiv.org/pdf/1512.02325.pdf) object detetion with Resnet50 backbone with feature pyramid network. You can choose other models, such as YOLO, feasterRCNN, etc. \n",
        "    \n",
        "    Note. For simplicity, a number of things in this notebook are harcoded for the specific RetinaNet architecture at hand, including assuming that the image size will always be 640x640. \n",
        "    \n",
        "    Another note. TensorFlow Hub is now the repository of trained machine learning models ([TensorFlow Hub Object Detection Colab tutorial](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)), however it seems that Tensorflow Hub models are not fine-tunable; see [issue](https://github.com/tensorflow/hub/issues/678)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc0OZaJmjR6d",
        "outputId": "7a948a1b-04bd-4e46-ff93-24b777b0a1af"
      },
      "outputs": [],
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uCuy8dbyF9X"
      },
      "source": [
        "2. Run the code below to load the pre-trained model.\n",
        "\n",
        "    One reason it is fairly complex is that the pre-trained model requires backward compatibility with TensorFlow 1.0 and because there is some added complexity for managing all the different pre-trained models in this particular repository. For some documentation on the simpler and cleaner semantics of saving and loading model checkpoints in TensorFlow 2.0 see [this documentation](https://www.tensorflow.org/tutorials/keras/save_and_load?authuser=2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvpReUumjR6e"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import config_util\n",
        "from object_detection.builders import model_builder\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "num_classes = 1\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "model_config.ssd.num_classes = num_classes\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=True)\n",
        "\n",
        "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "    #    (i.e., the classification head that we *will not* restore)\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "fake_model = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=fake_box_predictor)\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "# Run model through a dummy image so that variables are created\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2nd4Q23bLlA"
      },
      "source": [
        "3. Use the following code to make some predictions *before* fine-tuning. The model by default will generate 100 possible objects, each with associated confidence scores and predicted classes.\n",
        "\n",
        "    We utilize the `detect()` function, which wraps the preprocessing, prediction, and postprocessing step with the `tf.function` dectorator so that this computation will enjoy faster performance (see [this tutorial](https://www.tensorflow.org/guide/function) for more context)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6-ATyKz11QL"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that height and width can be anything since the image will be\n",
        "        immediately resized according to the needs of the model within this\n",
        "        function.\n",
        "\n",
        "    Returns:\n",
        "      A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "        and `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "    return detection_model.postprocess(prediction_dict, shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GBcqVDVu4jfi",
        "outputId": "0065655b-7ba7-44b9-bdd1-d33ce5df6107"
      },
      "outputs": [],
      "source": [
        "pre_ft_bb_preds = []\n",
        "pre_ft_scores_preds = []\n",
        "pre_ft_classes_preds = []\n",
        "label_id_offset = 1\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i in range(len(test_images_np)):\n",
        "    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "    detections = detect(input_tensor)\n",
        "    pre_ft_bb_preds.append(detections['detection_boxes'][0].numpy())\n",
        "    pre_ft_scores_preds.append(detections['detection_scores'][0].numpy())\n",
        "    pre_ft_classes_preds.append(detections['detection_classes'][0].numpy().astype(np.uint32) + label_id_offset)\n",
        "    if i < 5:\n",
        "        ax = plt.subplot(3, 2, i + 1)\n",
        "        ax.axes.xaxis.set_visible(False)\n",
        "        ax.axes.yaxis.set_visible(False)\n",
        "        temp_img_copy = np.copy(test_images_np[i][0])\n",
        "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            temp_img_copy,\n",
        "            pre_ft_bb_preds[-1],\n",
        "            pre_ft_classes_preds[-1],\n",
        "            pre_ft_scores_preds[-1],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            min_score_thresh=0.0\n",
        "        )\n",
        "        plt.imshow(temp_img_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SRxrJJVekvy"
      },
      "source": [
        "4. You can see that it makes some bad predictions (though the correct prediction is one of the possibilities). Let's just display its most confident prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r8kRja_7esk6",
        "outputId": "4b867bf6-59e9-4229-d29a-dfc27975fb01"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "for i in range(5):\n",
        "    ax = plt.subplot(3, 2, i + 1)\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    most_confident_bb = np.argmax(pre_ft_scores_preds[i])\n",
        "    temp_img_copy = np.copy(test_images_np[i][0])\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        temp_img_copy,\n",
        "        pre_ft_bb_preds[i][most_confident_bb][None, :],\n",
        "        np.array([pre_ft_classes_preds[i][most_confident_bb]]),\n",
        "        np.array([pre_ft_scores_preds[i][most_confident_bb]]),\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        min_score_thresh=0.0\n",
        "    )\n",
        "    plt.imshow(temp_img_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JjcPIjIuNXv"
      },
      "source": [
        "## Task 4: Implement Computation of Intersection Over Union metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31K-UbnVPbod"
      },
      "source": [
        "1. The [Intersection over Union](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) is one way to measure how *good* a bounding box prediction is.\n",
        "\n",
        "   Complete the `bounding_box_iou()` function below. You will need to complete these steps in the code:\n",
        "   \n",
        "   - Determine the coodinates of the intersection rectangle.\n",
        "   - Compute the area of the intersection rectangle.\n",
        "   - Compute the area of both the prediction and ground-truth rectangles.\n",
        "   - Compute and return the intersection over union by taking the intersection area and dividing it by the sum of the prediction and ground-truth areas minus the intersection area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtgwXe9j3uWb"
      },
      "outputs": [],
      "source": [
        "def bounding_box_iou(boxA:list, boxB:list) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Intersection over Union for two bounding boxes\n",
        "\n",
        "    Args:\n",
        "        boxA (ymin, xmin, ymax, xmax)\n",
        "        boxB (ymin, xmin, ymax, xmax)\n",
        "    \n",
        "    Return: intersection Over Union metric given two boxes cordinates\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lOydbj6Pboe"
      },
      "outputs": [],
      "source": [
        "assert bounding_box_iou([0,0,1,1], [0,0,1,1]) == 1\n",
        "assert bounding_box_iou([0,0,1,1], [1,1,2,2]) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLi4I1Rvk0lL"
      },
      "source": [
        "2. Use the `compute_best_iou_and_score()` function to find the best IoU and scores of the test images.\n",
        "\n",
        "    Remember the test images are stored in `t_gt_boxes`, the predicted bounding boxes are stored in `pre_ft_bb_preds` and the prediction scores per bounding box are stored in `pre_ft_scores_preds`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK_WrQNwj-7N"
      },
      "outputs": [],
      "source": [
        "def compute_best_iou_and_score(gt_bounding_box, predicted_bounding_boxes, prediction_scores_per_bb):\n",
        "    \"\"\"\n",
        "    compute the best IoU and scores\n",
        "    \"\"\"\n",
        "    indv_ious = []\n",
        "    for bbox_idx in range(len(predicted_bounding_boxes)):\n",
        "        indv_ious.append(bounding_box_iou(gt_bounding_box, predicted_bounding_boxes[bbox_idx]))\n",
        "    if len(indv_ious) == 0:\n",
        "        return 0, 0\n",
        "    best_bounding_box = np.argmax(indv_ious)\n",
        "    return indv_ious[best_bounding_box], prediction_scores_per_bb[best_bounding_box]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpBtbuFyl134",
        "outputId": "ab581174-b3dc-4f35-8911-dcc53182893a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ious = []\n",
        "scores = []\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print('The mean IoU on the test images is: ', np.mean(ious))\n",
        "print('The associated scores are: ', np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert np.isclose(np.mean(ious), .25, atol=5e-2) # Expect the mean IoU to be in the neighborhood of .25\n",
        "assert np.isclose(np.mean(scores), .03, atol=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wm73vq0m63F"
      },
      "source": [
        "## Task 5: Fine-tune the model\n",
        "\n",
        "1. The variables that we can train are located in the `.trainable_variables` attribute of the model `detection_model`. How many variables are there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9a9WIwnjR6e",
        "outputId": "36ae9ec5-cfa9-4ac4-b458-e52dddb8b931"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mwleEq9q9wJ"
      },
      "source": [
        "2. Print the names of all the trainable variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXaeGvXdq9wJ",
        "outputId": "44d5cae6-21dc-4e00-d2f5-fa6d8a221264"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ96mGv9nDef"
      },
      "source": [
        "We're going to fine tune the `WeightSharedConvolutionalBoxPredictor` layer only. Don't worry about why specifically this layer for the purposes of this tutorial. When you fine tune your own models, picking which parts of it to fine tune are a combination of the inductive bias you impose, and the result of hyperparameter optimization.\n",
        "\n",
        "3. Complete the `get_model_train_step_function()` function.\n",
        "\n",
        "    * Use the model's `.predict()` method to generate predictions and save as `prediction_dict`.\n",
        "    * Use the model's `.loss()` method and save as `losses_dict`.\n",
        "    * Make `total_loss` equal to the sum of `localization_loss` and `classification_loss` from the losses dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkxGHnR7q9wJ"
      },
      "outputs": [],
      "source": [
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "    \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "    # Use tf.function for a bit of speed.\n",
        "    # Comment out the tf.function decorator if you want the inside of the\n",
        "    # function to run eagerly.\n",
        "    @tf.function\n",
        "    def train_step_fn(image_tensors, groundtruth_boxes_list, groundtruth_classes_list):\n",
        "        \"\"\"A single training iteration.\n",
        "        Args:\n",
        "          image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "            Note that the height and width can vary across images, as they are\n",
        "            reshaped within this function to be 640x640.\n",
        "          groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "            tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "          groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "            with type tf.float32 representing groundtruth classes for each image in\n",
        "            the batch.\n",
        "\n",
        "        Returns:\n",
        "          A scalar tensor representing the total loss for the input batch.\n",
        "        \"\"\"\n",
        "        \n",
        "        shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
        "        model.provide_groundtruth(\n",
        "            groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "            groundtruth_classes_list=groundtruth_classes_list)\n",
        "        with tf.GradientTape() as tape:\n",
        "            preprocessed_images = tf.concat(\n",
        "                [detection_model.preprocess(image_tensor)[0]\n",
        "                for image_tensor in image_tensors], axis=0)\n",
        "            prediction_dict = # YOUR CODE HERE\n",
        "            losses_dict = # YOUR CODE HERE\n",
        "            total_loss = # YOUR CODE HERE\n",
        "            gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "            optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "        return total_loss\n",
        "    return train_step_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slfRP8yYq9wK"
      },
      "source": [
        "4. Set the tuning parameters. We give reasonable values, but feel free to adjust them and see how it affects the convergence of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yusTntkoq9wK"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "learning_rate = 0.01\n",
        "num_batches = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8DmWd7q9wK"
      },
      "source": [
        "5. Complete the following code by completing the `optimizer` and `train_step_fn`.\n",
        "\n",
        "    * Instantiate an SGD optimizer using the learning rate, and a momentum of 0.9 and save it as `optimizer`.\n",
        "    * Call the `get_model_train_step_function()` function to create `train_step_fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGQYHa0tjR6f"
      },
      "outputs": [],
      "source": [
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "        to_fine_tune.append(var)\n",
        "\n",
        "optimizer = # YOUR CODE HERE\n",
        "train_step_fn # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cViKVNW5q9wK"
      },
      "source": [
        "6. Run the following code to do the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rvjDlZdq9wK",
        "outputId": "78b659c3-6812-480b-b1ff-074d6d76b979"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "for idx in range(num_batches):\n",
        "    all_keys = list(range(len(train_images_np)))\n",
        "    random.shuffle(all_keys)\n",
        "    example_keys = all_keys[:batch_size]\n",
        "\n",
        "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "    total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "        print('batch ' + str(idx) + ' of ' + str(num_batches) + ', loss=' +  str(total_loss.numpy()), flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lYuzIU7o4aq"
      },
      "source": [
        "7. Now look at the bounding boxes for our fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pHDiizCco-HK",
        "outputId": "60e0cc20-a44c-457d-a2a8-81e8def8d5b7"
      },
      "outputs": [],
      "source": [
        "post_ft_bb_preds = []\n",
        "post_ft_scores_preds = []\n",
        "post_ft_classes_preds = []\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i in range(len(test_images_np)):\n",
        "    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "    detections = detect(input_tensor)\n",
        "    post_ft_bb_preds.append(detections['detection_boxes'][0].numpy())\n",
        "    post_ft_scores_preds.append(detections['detection_scores'][0].numpy())\n",
        "    post_ft_classes_preds.append(detections['detection_classes'][0].numpy().astype(np.uint32) + label_id_offset)\n",
        "    if i < 5:\n",
        "        ax = plt.subplot(3, 2, i + 1)\n",
        "        ax.axes.xaxis.set_visible(False)\n",
        "        ax.axes.yaxis.set_visible(False)\n",
        "        temp_img_copy = np.copy(test_images_np[i][0])\n",
        "        most_confident_bb = np.argmax(post_ft_scores_preds[i])\n",
        "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            temp_img_copy,\n",
        "            post_ft_bb_preds[i][most_confident_bb][None, :],\n",
        "            np.array([post_ft_classes_preds[i][most_confident_bb]]),\n",
        "            np.array([post_ft_scores_preds[i][most_confident_bb]]),\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            min_score_thresh=0.0\n",
        "        )\n",
        "        plt.imshow(temp_img_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEWkdLRKq9wL"
      },
      "source": [
        "8. Compute the mean IoU and scores for the fine_tuned model following the same steps as in task 4. The performance should be much better than the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_VsZi_1rDp9",
        "outputId": "b785d3ba-afca-47fc-ca4e-f8b0fb99d753",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ious = []\n",
        "scores = []\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print('The mean IoU on the test images is: ', np.mean(ious))\n",
        "print('The associated scores are: ', np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Npa2j9XPboh"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(np.mean(ious), .48, atol=1e-1)\n",
        "assert np.isclose(np.mean(scores), .03, atol=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXPE8pZfPboh"
      },
      "source": [
        "# References & Acknowledges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1aFLPnVPboh"
      },
      "source": [
        "- The notebook is adapted from [Tensorflow models](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb) (the demo).\n",
        "\n",
        "- SSD (original paper: [single shot multibox detector](https://arxiv.org/pdf/1512.02325.pdf))\n",
        "\n",
        "- [TensorFlow Hub Object Detection Colab](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)\n",
        "\n",
        "- [Deep Learning for Object Detection: A Comprehensive Review](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)\n",
        "\n",
        "- [Paper with code: Object Detection SOTA](https://paperswithcode.com/task/object-detection)\n",
        "\n",
        "\n",
        "- [How to Train Your Own Object Detector Using TensorFlow Object Detection API](https://neptune.ai/blog/how-to-train-your-own-object-detector-using-tensorflow-object-detection-api)\n",
        "\n",
        "- The assignment does not focus on image classification, the following three links are useful if you would like to quickly go over the fundamentals on the subject.\n",
        "  - [Convolutional neural networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "  - [Deep learning tips and tricks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)\n",
        "  - [The evolution of image classification explained](https://stanford.edu/~shervine/blog/evolution-image-classification-explained)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "few-shot-object-detection-solution.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.14 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "245.1875px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
