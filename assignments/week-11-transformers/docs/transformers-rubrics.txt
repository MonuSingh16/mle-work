Q: In the transformer architecture in the paper "Attention Is All You Need", 
   how does Multi-head Attention work?

A: https://arxiv.org/pdf/1706.03762.pdf  
Multi-head Attention uses several 'Scaled Dot-Product Attention' attention 
   layers running in parallel, enabling it to jointly attend to information
   from different representative subspaces at different positions. To do this
   it linearly projects the queries, keys, and values 'h' times from different
   learned linear projection to d_k,d_k,d_v dimensions respectively. On each of
   these projected versions of the queries, keys, and values, the attention 
   function is performed in parallel, giving d_v dimensional output values. 
   These are concatenated and once again projected to give final values.

Q: What is the main idea behind Positional Encoding?
A: Positional encoding describes the location or position of an entity in a sequence
   so that each position is assigned a unique representation. The main idea for 
   doing this is to make use of the order of the sequence so that context of knowing what
   words come before and after it and where it lies in a sentence 

Q: What is EarlyStopping and why do we use it?
A: 
 Early stopping is a method that allows you to specify an arbitrary large number of training epochs 
 and stop training once the model performance stops improving on a hold out validation dataset.
 We utilize Early Stopping to help prevent overfitting or put another way 'train the model enough to generalize
 but not enough to learn the statistical noise of the training dataset'. 

Q: How would explain what a transformer model is to business stakeholders (at a high level)?
A: A transformer model is a neural network that learns context and thus meaning by tracking 
relationships in sequential data like the words in this sentence. 
Transformer takes in a sequence of data, uses an encoder to turn it into code, 
then the decoder learns how to reconstruct the encoded data into some desired output sequence. 
This could a translation from one language to another, text summarization, text generation.