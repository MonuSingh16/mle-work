{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In Class Follow Along Notebook\n"
      ],
      "metadata": {
        "id": "-egkpUTSarLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first, we'll set-up the data!"
      ],
      "metadata": {
        "id": "UJjtovQ9axry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i_bITLVY7zD"
      },
      "outputs": [],
      "source": [
        "NUM_LABELS = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "cleaned_tweets = pd.read_csv(\"processed_tweets (1).csv\")"
      ],
      "metadata": {
        "id": "OkIZ8Z6oZEKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets.head()"
      ],
      "metadata": {
        "id": "wP0EBj57ZZiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = pd.Series(cleaned_tweets['tidy_tweet']), pd.Series(cleaned_tweets['label'])"
      ],
      "metadata": {
        "id": "YNdI8lPqZfBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_sub, X_test, y_train_sub, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "mckqeu1aZpfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sub, y_train_sub, test_size=0.2, stratify=y_train_sub, random_state=42)"
      ],
      "metadata": {
        "id": "VTWa4CbkaAeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding Layer\n",
        "\n",
        "We'll make the positional embedding layer as seen in the \"Attention is all you need\" paper!\n",
        "\n",
        "The idea behind Positional Encoding is fairly simple as well: to give the model access to token order information, therefore we are going to add the token's position in the sentence to each word embedding.\n",
        "\n",
        "Thus, one input word embedding will have two components: the usual token vector representing the token independent of any specific context, and a position vector representing the position of the token in the current sequence."
      ],
      "metadata": {
        "id": "990gUduTa2a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Positional Embedding\n",
        "from tensorflow.keras import layers as L\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class PositionalEmbedding(L.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        self.token_embeddings =  # YOUR CODE HERE\n",
        "        self.position_embeddings =  # YOUR CODE HERE\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "CPEA_KsZaky5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block\n",
        "\n",
        "Recently most of the natural language processing tasks are being dominated by the Transformer architecture, introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762), which used a simple mechanism called Neural Attention as one of its building blocks. As the title suggests this architecture didn't require any recurrent layer. We now build a text classification using Attention and Positional Embeddings.\n",
        "\n",
        "Transformer (attention) Block.\n",
        "\n",
        "The concept of Neural Attention is fairly simple; i.e., not all input information seen by a model is equally important to the task at hand. Although this concept has been utilized at various different places as well, e.g., max pooling in ConvNets, but the kind of attention we are looking for should be context aware.\n",
        "\n",
        "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other; in other words, calculate attention of all other inputs with respect to one input.\n",
        "\n",
        "In the paper, the authors proposed another type of attention mechanism called multi-headed attention which refers to the fact that the outer space of the self attention layer gets factored into a set of independent sub-spaces learned separately, where each subspace is called a \"head\". You need to implement the multi-head attention layer, supplying values for two parameters: num_heads and key_dim.\n",
        "\n",
        "There is a learnable dense projection present after the multi-head attention which enables the layer to actually learn something, as opposed to being a purely stateless transformation. You need to implement dense_proj, use the tf.keras.Sequential to stack two dense layers:\n",
        "\n",
        " 1. first dense layer with `dense_dim` units and activation function `relu`;\n",
        " 2. second dense layer with `embed_dim` units and no activation function."
      ],
      "metadata": {
        "id": "AzCOplI_eJEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(L.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention =  # YOUR CODE HERE\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            L.Dense(dense_dim, activation='relu'),\n",
        "            L.Dense(embed_dim)\n",
        "            ])\n",
        "        self.layernorm1 = L.LayerNormalization()\n",
        "        self.layernorm2 = L.LayerNormalization()\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[: tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm2(proj_input + proj_output)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "mSQCJSvTecQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model in Keras\n",
        "\n",
        "Let's build it!"
      ],
      "metadata": {
        "id": "mGlaqhELc4XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10_000\n",
        "EMBED_DIM = 256\n",
        "DENSE_DIM = 32\n",
        "NUM_HEADS = 2\n",
        "MAX_LEN = 256"
      ],
      "metadata": {
        "id": "kGU1mvM5dBCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer.\n",
        "\n",
        "The tokenizer is a simple tool to convert a text into a sequence of tokens. It is used to convert the training data into a sequence of integers, which are then used as input to the model.\n",
        "\n",
        "Use Tokenizer to create a tokenizer for the training data. Set the num_words parameter to the number of words to keep in the vocabulary, and oov_token to be \"\\<unk>\"."
      ],
      "metadata": {
        "id": "-FQNdZcOdEwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = # YOUR CODE HERE\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "FpvW57zwdCrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad the sequences.\n",
        "\n",
        "The tokenizer outputs a sequence of integers, which are then used as input to the model. However, the model expects a sequence of fixed length. To pad the sequences to the same length, use sequence.pad_sequences from keras.preprocessing.\n",
        "\n",
        "Complete function preprocess below to 1) tokenize the texts 2) pad the sequences to the same length."
      ],
      "metadata": {
        "id": "_qJJ073cdMuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "def preprocess(texts, tokenizer, maxlen:int = MAX_LEN):\n",
        "    seqs =  # YOUR CODE HERE\n",
        "    tokenized_text =   # YOUR CODE HERE\n",
        "    return tokenized_text"
      ],
      "metadata": {
        "id": "ABEZtL1fdNVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data.\n",
        "\n",
        "Use preprocess to preprocess the training, validation, and test data."
      ],
      "metadata": {
        "id": "oFMp4gygdQXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train =  # YOUR CODE HERE\n",
        "X_valid =  # YOUR CODE HERE\n",
        "X_test  =  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "M_64X4SUdRWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model with the following architecture:\n",
        "\n",
        "* Input Layer\n",
        "* Positional Embeddings\n",
        "* Transformer Block\n",
        "* Pooling\n",
        "* Dropout\n",
        "* Output Layer\n",
        "\n",
        "If you are not familiar with keras functional API, take a read [here](https://keras.io/guides/functional_api/)."
      ],
      "metadata": {
        "id": "DEw_iUdLdVod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "x = PositionalEmbedding(MAX_LEN, VOCAB_SIZE, EMBED_DIM)(inputs) # YOUR CODE HERE\n",
        "x = TransformerBlock(EMBED_DIM, DENSE_DIM, NUM_HEADS)(x) # YOUR CODE HERE\n",
        "x = L.GlobalMaxPooling1D()(x)\n",
        "x = L.Dropout(0.1)(x)\n",
        "outputs = L.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "nj6VLLiRdW3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile model.\n",
        "\n",
        "Use 'adam' for the optimizer and accuracy for metrics, supply the correct value for loss.\n",
        "\n",
        "Remember, this is a binary classification task!"
      ],
      "metadata": {
        "id": "2vBkk6dAdox1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam', # YOUR CODE HERE\n",
        "    loss='binary_crossentropy', # YOUR CODE HERE\n",
        "    metrics=['accuracy']) # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "D0H-JOY7dpa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "u_noMiW2dss4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) and [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to stop training if the model does not improve a set metric after a given number of epochs.\n",
        "\n",
        "Create an EarlyStopping object named es to stop training if the validation loss does not improve after 5 epochs. Set verbose to display messages when the callback takes an action and set restore_best_weights to restore model weights from the epoch with the best value of the monitored metric.\n",
        "\n",
        "Use ReduceLROnPlateau to reduce the learning rate if the validation loss does not improve after 3 epochs. Set verbose to display messages when the callback takes an action and use default values for other parameters."
      ],
      "metadata": {
        "id": "1mf4WnvTdu2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es =  # YOUR CODE HERE\n",
        "rlp =  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "A221divwd2pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model.\n",
        "\n",
        "Supply both EarlyStopping and ReduceLROnPlateau for callbacks. Set epochs to 100."
      ],
      "metadata": {
        "id": "prI5VSd5d5ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train, \n",
        "    validation_data=(X_valid, y_valid),\n",
        "    # YOUR CODE HERE\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "rxTZtZ_nd8Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the trained model on the test data."
      ],
      "metadata": {
        "id": "bCs1ljq5d-fl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yk20ucAWd_hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize both loss and accuracy curves for the training and validation data."
      ],
      "metadata": {
        "id": "Vpr4AktheCHj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-UQh-ADKeDUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}