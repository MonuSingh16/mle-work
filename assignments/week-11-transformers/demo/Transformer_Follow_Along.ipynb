{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egkpUTSarLK"
      },
      "source": [
        "# In Class Follow Along Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJjtovQ9axry"
      },
      "source": [
        "First things first, we'll set-up the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2i_bITLVY7zD"
      },
      "outputs": [],
      "source": [
        "NUM_LABELS = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "cleaned_tweets = pd.read_csv(\"cleaned_tweets.csv\")"
      ],
      "metadata": {
        "id": "OkIZ8Z6oZEKp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cleaned_tweets = pd.read_csv(\"cleaned_tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wP0EBj57ZZiw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tidy_tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when a father is dysfunctional and is so sel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thanks for #lyft credit i cant use cause the...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>factsguide society now    #motivation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tidy_tweet  label\n",
              "0    when a father is dysfunctional and is so sel...      0\n",
              "1    thanks for #lyft credit i cant use cause the...      0\n",
              "2                                bihday your majesty      0\n",
              "3  #model   i love u take with u all the time in ...      0\n",
              "4              factsguide society now    #motivation      0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YNdI8lPqZfBK"
      },
      "outputs": [],
      "source": [
        "X, y = pd.Series(cleaned_tweets['tidy_tweet']), pd.Series(cleaned_tweets['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mckqeu1aZpfN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_sub, X_test, y_train_sub, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VTWa4CbkaAeD"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sub, y_train_sub, test_size=0.2, stratify=y_train_sub, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990gUduTa2a5"
      },
      "source": [
        "## Positional Embedding Layer\n",
        "\n",
        "We'll make the positional embedding layer as seen in the \"Attention is all you need\" paper!\n",
        "\n",
        "The idea behind Positional Encoding is fairly simple as well: to give the model access to token order information, therefore we are going to add the token's position in the sentence to each word embedding.\n",
        "\n",
        "Thus, one input word embedding will have two components: the usual token vector representing the token independent of any specific context, and a position vector representing the position of the token in the current sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CPEA_KsZaky5"
      },
      "outputs": [],
      "source": [
        "### Positional Embedding\n",
        "from tensorflow.keras import layers as L\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class PositionalEmbedding(L.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        self.token_embeddings =  L.Embedding(input_dim=input_dim, output_dim=output_dim) # token embeddings layer\n",
        "        self.position_embeddings =  L.Embedding(input_dim=sequence_length, output_dim=output_dim) # position of tokens embeddings layer\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzCOplI_eJEf"
      },
      "source": [
        "## Transformer Block\n",
        "\n",
        "Recently most of the natural language processing tasks are being dominated by the Transformer architecture, introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762), which used a simple mechanism called Neural Attention as one of its building blocks. As the title suggests this architecture didn't require any recurrent layer. We now build a text classification using Attention and Positional Embeddings.\n",
        "\n",
        "Transformer (attention) Block.\n",
        "\n",
        "The concept of Neural Attention is fairly simple; i.e., not all input information seen by a model is equally important to the task at hand. Although this concept has been utilized at various different places as well, e.g., max pooling in ConvNets, but the kind of attention we are looking for should be context aware.\n",
        "\n",
        "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other; in other words, calculate attention of all other inputs with respect to one input.\n",
        "\n",
        "In the paper, the authors proposed another type of attention mechanism called multi-headed attention which refers to the fact that the outer space of the self attention layer gets factored into a set of independent sub-spaces learned separately, where each subspace is called a \"head\". You need to implement the multi-head attention layer, supplying values for two parameters: num_heads and key_dim.\n",
        "\n",
        "There is a learnable dense projection present after the multi-head attention which enables the layer to actually learn something, as opposed to being a purely stateless transformation. You need to implement dense_proj, use the tf.keras.Sequential to stack two dense layers:\n",
        "\n",
        " 1. first dense layer with `dense_dim` units and activation function `relu`;\n",
        " 2. second dense layer with `embed_dim` units and no activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mSQCJSvTecQ0"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(L.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        # key_dim = Size of each attention head for query and key. # num_heads = Number of attention heads.\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            L.Dense(dense_dim, activation='relu'),\n",
        "            L.Dense(embed_dim)\n",
        "            ])\n",
        "        self.layernorm1 = L.LayerNormalization()\n",
        "        self.layernorm2 = L.LayerNormalization()\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[: tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm2(proj_input + proj_output)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlaqhELc4XJ"
      },
      "source": [
        "## Transformer Model in Keras\n",
        "\n",
        "Let's build it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kGU1mvM5dBCR"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10_000\n",
        "EMBED_DIM = 256\n",
        "DENSE_DIM = 32\n",
        "NUM_HEADS = 2\n",
        "MAX_LEN = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FQNdZcOdEwf"
      },
      "source": [
        "Tokenizer.\n",
        "\n",
        "The tokenizer is a simple tool to convert a text into a sequence of tokens. It is used to convert the training data into a sequence of integers, which are then used as input to the model.\n",
        "\n",
        "Use Tokenizer to create a tokenizer for the training data. Set the num_words parameter to the number of words to keep in the vocabulary, and oov_token to be \"\\<unk>\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FpvW57zwdCrW"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE,oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qJJ073cdMuF"
      },
      "source": [
        "Pad the sequences.\n",
        "\n",
        "The tokenizer outputs a sequence of integers, which are then used as input to the model. However, the model expects a sequence of fixed length. To pad the sequences to the same length, use sequence.pad_sequences from keras.preprocessing.\n",
        "\n",
        "Complete function preprocess below to 1) tokenize the texts 2) pad the sequences to the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ABEZtL1fdNVP"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "def preprocess(texts, tokenizer, maxlen:int = MAX_LEN):\n",
        "    seqs =  tokenizer.texts_to_sequences(texts) # tokenize texts\n",
        "    tokenized_text = pad_sequences(seqs, maxlen=maxlen) # pad texts to make each sequence the same length\n",
        "    return tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFMp4gygdQXr"
      },
      "source": [
        "Preprocess the data.\n",
        "\n",
        "Use preprocess to preprocess the training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M_64X4SUdRWt"
      },
      "outputs": [],
      "source": [
        "X_train = preprocess(X_train,tokenizer,MAX_LEN) # preprocess train data\n",
        "X_valid = preprocess(X_valid,tokenizer,MAX_LEN) # preprocess validation data\n",
        "X_test  =preprocess(X_test,tokenizer,MAX_LEN) # preprocess test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEw_iUdLdVod"
      },
      "source": [
        "Define the model with the following architecture:\n",
        "\n",
        "* Input Layer\n",
        "* Positional Embeddings\n",
        "* Transformer Block\n",
        "* Pooling\n",
        "* Dropout\n",
        "* Output Layer\n",
        "\n",
        "If you are not familiar with keras functional API, take a read [here](https://keras.io/guides/functional_api/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nj6VLLiRdW3u"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "x = PositionalEmbedding(MAX_LEN, VOCAB_SIZE, EMBED_DIM)(inputs) # YOUR CODE HERE\n",
        "x = TransformerBlock(EMBED_DIM, DENSE_DIM, NUM_HEADS)(x) # YOUR CODE HERE\n",
        "x = L.GlobalMaxPooling1D()(x)\n",
        "x = L.Dropout(0.1)(x)\n",
        "outputs = L.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vBkk6dAdox1"
      },
      "source": [
        "Compile model.\n",
        "\n",
        "Use 'adam' for the optimizer and accuracy for metrics, supply the correct value for loss.\n",
        "\n",
        "Remember, this is a binary classification task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D0H-JOY7dpa8"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam', # YOUR CODE HERE\n",
        "    loss='binary_crossentropy', # YOUR CODE HERE\n",
        "    metrics=['accuracy']) # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u_noMiW2dss4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posit  (None, None, 256)        2625536   \n",
            " ionalEmbedding)                                                 \n",
            "                                                                 \n",
            " transformer_block (Transfor  (None, None, 256)        543776    \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,169,569\n",
            "Trainable params: 3,169,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mf4WnvTdu2s"
      },
      "source": [
        "Add [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) and [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to stop training if the model does not improve a set metric after a given number of epochs.\n",
        "\n",
        "Create an EarlyStopping object named es to stop training if the validation loss does not improve after 5 epochs. Set verbose to display messages when the callback takes an action and set restore_best_weights to restore model weights from the epoch with the best value of the monitored metric.\n",
        "\n",
        "Use ReduceLROnPlateau to reduce the learning rate if the validation loss does not improve after 3 epochs. Set verbose to display messages when the callback takes an action and use default values for other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "A221divwd2pX"
      },
      "outputs": [],
      "source": [
        "es = keras.callbacks.EarlyStopping(patience=5,verbose=1,restore_best_weights=True) # create early stopping object\n",
        "rlp = keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1) # reduce learning rate once hit plateau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prI5VSd5d5ju"
      },
      "source": [
        "Train the model.\n",
        "\n",
        "Supply both EarlyStopping and ReduceLROnPlateau for callbacks. Set epochs to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rxTZtZ_nd8Cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-26 01:28:17.788840: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "560/560 [==============================] - 432s 768ms/step - loss: 0.1862 - accuracy: 0.9425 - val_loss: 0.1260 - val_accuracy: 0.9558 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "560/560 [==============================] - 349s 624ms/step - loss: 0.0683 - accuracy: 0.9766 - val_loss: 0.1494 - val_accuracy: 0.9571 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "560/560 [==============================] - 328s 586ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.1840 - val_accuracy: 0.9569 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "560/560 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9979\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "560/560 [==============================] - 351s 628ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.2105 - val_accuracy: 0.9553 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "560/560 [==============================] - 348s 621ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.1870 - val_accuracy: 0.9575 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "560/560 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998Restoring model weights from the end of the best epoch: 1.\n",
            "560/560 [==============================] - 321s 572ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2086 - val_accuracy: 0.9566 - lr: 1.0000e-04\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train, \n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks= [es,rlp], # set callbacks\n",
        "    epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs1ljq5d-fl"
      },
      "source": [
        "Evaluate the trained model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk20ucAWd_hZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpr4AktheCHj"
      },
      "source": [
        "Visualize both loss and accuracy curves for the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UQh-ADKeDUD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('venv-ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1dfdebb56f854d6d8d1fe2606ca3baeb29b4143fe0df5eb6f2cb885741a890e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
