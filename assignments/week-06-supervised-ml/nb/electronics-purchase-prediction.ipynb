{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9MqC-OwPKyQ"
   },
   "source": [
    "# Eletronics Purchase Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9MqC-OwPKyQ"
   },
   "source": [
    "Today you are a machine learning engineer in the Department of Marketing and Inventory at Walmart Labs. You have access to the Walmart server data, specifically the Electronics section. However, there is no customer facing information, but you do have access to timestamped data regarding product viewing/carting/purchasing. We will use this data to build a model of whether or not some product will be purchased.\n",
    "\n",
    "Data is adapted from [e-commerce behavior data on Kaggle](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store). \n",
    "\n",
    "This file contains behavior data from a large multi-category store. Each row in the file represents an event. All events are related to products and users. Each event has a many-to-many relationship between users and products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this session, you will be able to\n",
    "\n",
    "- Leverage `pandas_profiling` for fast data understanding\n",
    "- Practice data preprocessing\n",
    "- Build logistic regression / SVM / Gradient Boosting\n",
    "- Evaluate models with proper metrics\n",
    "- Interpret black box models with SHAP\n",
    "- Generate optimal pipeline for classification task using AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dR-vbsrRTv_"
   },
   "source": [
    "## Task 1: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dR-vbsrRTv_"
   },
   "source": [
    "Let's start by looking at the descriptions before loading in the csv files.\n",
    "\n",
    "1. Use the `IPython.display` module to view the `some_column_descriptions.png` file. Look through the column names and descriptions to get an idea of what the data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "executionInfo": {
     "elapsed": 1253,
     "status": "ok",
     "timestamp": 1628175773190,
     "user": {
      "displayName": "Spencer Kent",
      "photoUrl": "",
      "userId": "07841346171340846448"
     },
     "user_tz": 360
    },
    "id": "h9EE7PccRRc9",
    "outputId": "d0c4ead6-d26a-465a-ea5f-140913936192"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# change the filename to wherever you downloaded/uploaded the file\n",
    "filename = '../img/some_column_descriptions.png'\n",
    "display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzZiHeyvPKyW"
   },
   "source": [
    "The dataset has User-journey data, i.e., it tracks information on user/product pairs over time to see if the combination results in a purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzZiHeyvPKyW"
   },
   "source": [
    "2. Look at the `user_journey_descriptions.png` file. Review the data sample to get a sense of what information we are tracking for each user/product pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1628175803603,
     "user": {
      "displayName": "Spencer Kent",
      "photoUrl": "",
      "userId": "07841346171340846448"
     },
     "user_tz": 360
    },
    "id": "MU87_Tt-TLff",
    "outputId": "629943bc-95f5-4d92-a88a-697dc7b75535"
   },
   "outputs": [],
   "source": [
    "# change path to wherever you uploaded/downloaded the file\n",
    "filename='../img/user_journey_descriptions.png'\n",
    "display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSNM0etdU6HR"
   },
   "source": [
    "The dataset we are working with is essentially what we have screenshotted above, but has been anonymized by removing product IDs and user IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSNM0etdU6HR"
   },
   "source": [
    "3. Use the pandas `read_csv()` and `head()` functions to read in the training data (`../dat/train.csv.gz`) and look at the first few rows.\n",
    "\n",
    "    Note the `Purchase` column has values either 0 (not purchased) or 1 (purchased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the dimension of the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Similarly, read in the test data `../dat/test.csv.gz` and check its dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = None # YOUR CODE HERE\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JunsLTrHPKyZ"
   },
   "source": [
    "## Task 2: Understand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JunsLTrHPKyZ"
   },
   "source": [
    "Our goal here is to predict whether a customer/product pair will result in a purchase. \n",
    "To do this, we will want to identify the most important features for classification. \n",
    "\n",
    "Before digging into each column, let's leverage `pandas_profiling`, that goes beyond `pd.DataFrame.describe()`, to get a big picture of our data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From `pandas_profiling` import `ProfileReport`. \n",
    "    \n",
    "    For faster iteration, create a profile report object `profile` with 10,000 samples from `train_df`. \n",
    "    \n",
    "    Set its `title` as \"Pandas Profiling Electronics Purchase Report\". \n",
    "    \n",
    "    For sampling, use `pd.DataFrame.sample(10000, randome_state=3)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "profile = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Show the report inside the notebook, go over the report and check each tab; especially the \"Alerts\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xee_TNNyYHSy"
   },
   "source": [
    "2. Pandas profiling Profile reports that there are 5 numeric variables and 11 categorical variables. Let's print out the datatype of each feature (column) and understand how the profiler infers data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2QfKLnzYXqA"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. From the report, we see that `year`, `month`, and `Weekend` were \"Rejected\" as there is only one distinct value for each column. \n",
    "These features are not informative, so we remove the three features from the training set. To do this, pass the `columns` argument to the `.drop()` method. \n",
    "Make sure to use `inplace=True` to modify the DataFrame. \n",
    "Print the shape of the DataFrames to verify the columns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df.shape[1] == 15\n",
    "# YOUR CODE HERE\n",
    "assert train_df.shape[1] == 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Repeat the steps for `test_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "assert test_df.shape[1] == train_df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LzuyL8gZXNy"
   },
   "source": [
    "5. As you can see from the profile report, `weekday` and `timeOfDay` each has 7 distinct values. \n",
    "Convert these non-numeric features to numeric. \n",
    "\n",
    "    These feature values are ordered temporally, so it makes sense to convert them to numeric type. \n",
    "    Follow the example given for the `weekday` column to update the `timeOfDay` column.\n",
    "    Use the `.head()` method to inspect the dataset after the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN503xCOPKyf"
   },
   "outputs": [],
   "source": [
    "weekday_str2num = {\n",
    "    s: i+1 for i, s in enumerate(['Mon', 'Tue', 'Wed', 'Thu', 'Fr', 'Sat', 'Sun'])\n",
    "}\n",
    "train_df['weekday'] = train_df['weekday'].replace(weekday_str2num)\n",
    "train_df['weekday'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1eye5KePKym"
   },
   "outputs": [],
   "source": [
    "timeOfDay_str2num = None # YOUR CODE HERE\n",
    "train_df['timeOfDay'] = None # YOUR CODE HERE\n",
    "train_df['timeOfDay'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Do the same for `test_df`, that is, to convert `weekday` and `timeOfDay` into numeric type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['weekday'] = None # YOUR CODE HERE\n",
    "test_df['timeOfDay'] = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use the `Purchase` column in training set to determine the proportion of user journeys that result in purchases.  Compare it to what the profile reports. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mab-Q3lKPKyo"
   },
   "outputs": [],
   "source": [
    "print(\"number of purchases vs non-purchases in train set:\")\n",
    "print(train_df.Purchase.value_counts())\n",
    "print(f\"percent of rows resulting in purchase: {None}\") # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHkECAjyPKyq"
   },
   "source": [
    "## Task 3: Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation), but they affect specific models in different ways and to varying extents:\n",
    "\n",
    "- For linear models (e.g., linear regression or logistic regression), [multicolinearity](https://en.wikipedia.org/wiki/Multicollinearity) can yield [solutions that are wildly varying and possibly numerically unstable](https://en.wikipedia.org/wiki/Multicollinearity#Consequences).\n",
    "- Random forest can be good at detecting interactions between different features, but highly correlated features can mask these interactions.\n",
    "More generally, this can be viewed as a special case of [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor). A simpler model is preferable, and, in some sense, a model with fewer features is simpler. The concept of [minimum description length](https://en.wikipedia.org/wiki/Minimum_description_length) makes this more precise ([ref](https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHkECAjyPKyq"
   },
   "source": [
    "1. How many features does our dataset currently have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_4jlj1DPKyq"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV2VP1qRcU1O"
   },
   "source": [
    "2. Drop the features with high correlation.\n",
    "\n",
    "    In the \"Correlations\" tab in the profile report, you can find heatmap using five different correlation metrics; toggle correlation desciptions if any one of them looks foreign to you.  \n",
    "\n",
    "    We'll remove high correlated features by looking at each pair of features, and if they are highly correlated (at least 0.8), we won't include the second feature in the pair. Store the remaining set of features (the ones you didn't drop) in dataframes `train_df_reduced`. You are given most the code, make sure that you understand what each line does.\n",
    "    \n",
    "    First, calculate the correlation with just 10,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df = train_df.copy().sample(n=int(1e4), random_state=12) \n",
    "cor = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kAcOhfzPKyx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "keep_columns = np.full(cor.shape[0], True)\n",
    "for i in range(cor.shape[0] - 1):\n",
    "    for j in range(i + 1, cor.shape[0] - 1):\n",
    "        if (np.abs(cor.iloc[i, j]) >= 0.8):\n",
    "            keep_columns[j] = False\n",
    "selected_columns = train_df.columns[keep_columns]\n",
    "train_df_reduced = train_df[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(selected_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many columns are left in the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_reduced.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove the same features from the test set and save in a new dataframe `test_df_reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_reduced = None # YOUR CODE HERE\n",
    "assert test_df_reduced.shape[1] == 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytOeqOQOd8Fu"
   },
   "source": [
    "5. Visualize the selected features and discuss your observations with your team.\n",
    "\n",
    "    Again, for faster rendering, use the subset `train_small_df_reduced`. If time permits, experiment with some other visualizations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zr7RYNyPKy5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "train_small_df_reduced = train_small_df[selected_columns]\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "j = 0\n",
    "for i in train_df_reduced.columns:\n",
    "    plt.subplot(3, 4, j + 1)\n",
    "    j += 1\n",
    "    sns.histplot(train_small_df_reduced[i][train_small_df_reduced['Purchase'] == 0], color='g', label='no')\n",
    "    sns.histplot(train_small_df_reduced[i][train_small_df_reduced['Purchase'] == 1], color='r', label='yes')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Subscription Feature Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make the NumPy arrays `X_train_reduced`, `X_test_reduced`, `y_train` and `y_test` from `train_df_reduced` and `test_df_reduced`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df_reduced['Purchase'].values\n",
    "y_test = None # YOUR CODE HERE\n",
    "X_train_reduced = train_df_reduced.drop(columns='Purchase').values\n",
    "X_test_reduced = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Scale the features in `X_train_reduced` and `X_test_reduced` use `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = None # YOUR CODE HERE\n",
    "X_train_reduced = None # YOUR CODE HERE\n",
    "X_test_reduced = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqG60zCOPKzF"
   },
   "source": [
    "## Task 4: Build Logistic Regression and SVM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqG60zCOPKzF"
   },
   "source": [
    "We will be fitting both a Logistic Regression and SVM model to the reduced features and then looking at classification metrics such as Accuracy, Precision, Recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecEMOdxyPKzG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score as accuracy,\n",
    "    recall_score as recall,\n",
    "    precision_score as precision,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qTOqA8oz107"
   },
   "source": [
    "1. Instantiate, train, and predict using the Logistic Regression model.\n",
    "\n",
    "    Make sure to account for the imbalanced classes with with `class_weight` parameter (what happens if we don't?).\n",
    "\n",
    "    Remember to use the ***train*** data for training the model and the ***test*** data when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh5SBHJrPKzI"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lr_model = None # YOUR CODE HERE\n",
    "lr_pred = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5iHJRIC0Nfl"
   },
   "source": [
    "2. Calculate these classification metrics for the Logistic Regression model:\n",
    "\n",
    "    * accuracy\n",
    "    * precision\n",
    "    * recall\n",
    "    * f1 score\n",
    "    * confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHshtF_IPKzP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"accuracy:\", accuracy(y_test, lr_pred))\n",
    "print(\"precision:\", precision(y_test, lr_pred))\n",
    "print(\"recall:\", recall(y_test, lr_pred))\n",
    "print(\"f1 score:\", f1_score(y_test, lr_pred))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv858jlV1d__"
   },
   "source": [
    "3. Instantiate, train and predict using the SVM model. Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) for usage. \n",
    "\n",
    "    Again, remember to account for the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1BXMFM2PKzR"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "svm_model = None # YOUR CODE HERE \n",
    "svm_model.fit(X_train_reduced, y_train)\n",
    "svm_pred = svm_model.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ❓ What is the time complexity of SVM? What is it for Logistic Regression? Not familiar with the term \"time complexity\"? Take a read on [Computational Complexity of ML Models\n",
    "](https://medium.com/analytics-vidhya/time-complexity-of-ml-models-4ec39fad2770)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4QY6-p21o1X"
   },
   "source": [
    "5. Calculate the classification metrics for the SVM model. \n",
    "\n",
    "Here you can use a helper function to display all the metrics. \n",
    "\n",
    "Inspect the source code to understand how to use the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_metrics\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's impressive! Why do you think SVM performs so well? If you are not familiar with Support Vector Machine, check [In-Depth: Support Vector Machines](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb) out for better understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Explanability with SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZoEc46tPKzZ"
   },
   "source": [
    "Intepretation of a linear model such as logistic regression is staightforward, however, some tree-based models have a different reputation. In this task, we will fit a non-linear classifier: begin with gradient boosted tree, and use SHAP to help intepret the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are training coefficients from logistic regression? ( how about SVM? check [How does one interpret SVM feature weights?](https://stats.stackexchange.com/questions/39243/how-does-one-interpret-svm-feature-weights) )\n",
    "\n",
    "    Leave your comments on interpretation of this logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZoEc46tPKzZ"
   },
   "source": [
    "2. Instantiate, train, and predict using the Gradient Boosted Trees model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhRgpHtyGnzD"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhRgpHtyGnzD"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gbt_model = None # YOUR CODE HERE\n",
    "gbt_model.fit(X_train_reduced, y_train)\n",
    "gbt_pred = gbt_model.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLalaVS0rHJ9"
   },
   "source": [
    "3. Evaluate the model by calculating the classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7fQY3dAGnzD"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SHAP (SHapley Additive exPlanations) is a game theory approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. You can use package [SHAP](https://github.com/slundberg/shap). \n",
    "\n",
    "    For this task, if you are not familar with the concept, read the book chapter [Interpretable Machine Learning - A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/shap.html) first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Consider experimenting on a smaller dataset. \n",
    "\n",
    "For example, start with a subset of 100 samples from the test data.\n",
    "\n",
    "Once the script works properly, use all the data from `X_test_reduced`. \n",
    "\n",
    "Apply `shap.Explainer()` to calculate the SHAP values for the Gradient Boosting classifier you fit earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "subset_size = X_test_reduced.shape[0] # start with 100\n",
    "idx = np.random.randint(X_test_reduced.shape[0], size=subset_size)\n",
    "X_test_reduced_subset = X_test_reduced[idx, ]\n",
    "\n",
    "assert X_test_reduced_subset.shape == (subset_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = None# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shap_values = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. visualize the effects over all samples using `shap.plots.beeswarm()` or `shap.summary_plot()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What are features 4, 3, and 1? Does each feature positively or negatively contribute to the likelihood of purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Visualize the effects over all samples using `shap.plots.beeswarm()` or `shap.summary_plot()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Write your data (shap values) to a file and add them under the Model Explainability tab in the Streamlit app (see next section), following the example in Model Results tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Optional. Do the same for a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how AutoML performs on this imbalanced dataset. Similarly, for faster iteration on our workflow, experiment it on a small smaple. \n",
    "\n",
    "1. Take 5,000 samples from `train_df`, 5,000 from `test_df` for experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_df.sample(int(5e3), random_state=42)\n",
    "train_label = train_features.pop('Purchase')\n",
    "test_features = test_df.sample(int(5e3), random_state=42)\n",
    "test_label = test_features.pop('Purchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Instantiate and train a TPOT auto-ML classifier.\n",
    "\n",
    "    The parameters in the code below are set to low values for faster iterations. \n",
    "    Read about each parameters [here](http://epistasislab.github.io/tpot/api/#classification) and experiment with different values when time permits.\n",
    "    \n",
    "    Set a proper [`scoring` function](http://epistasislab.github.io/tpot/using/#scoring-functions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=10, \n",
    "                      population_size=16,\n",
    "                      scoring=None,# YOUR CODE HERE\n",
    "                      verbosity=2,\n",
    "                      random_state=42)\n",
    "tpot.fit(train_features, train_label)\n",
    "print(f\"Tpop score on test data: {tpot.score(test_features, test_label):.2f}\")\n",
    "tpot.export('tpot_electronics_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Examine the model pipeline that TPOT classifier offers in the python file `tpot_electronics_pipeline.py` in the `nb` directory. \n",
    "If you see any model, function, or class that are not familiar, look them up!\n",
    "\n",
    "    Note: There is randomness to the way the TPOT searches, so it's possible you won't have exactly the same result as your classmate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "from sklearn.decomposition import FastICA\r\n",
      "from sklearn.ensemble import ExtraTreesClassifier\r\n",
      "from sklearn.model_selection import train_test_split\r\n",
      "from sklearn.pipeline import make_pipeline\r\n",
      "from tpot.export_utils import set_param_recursive\r\n",
      "\r\n",
      "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\r\n",
      "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\r\n",
      "features = tpot_data.drop('target', axis=1)\r\n",
      "training_features, testing_features, training_target, testing_target = \\\r\n",
      "            train_test_split(features, tpot_data['target'], random_state=42)\r\n",
      "\r\n",
      "# Average CV score on the training set was: 1.0\r\n",
      "exported_pipeline = make_pipeline(\r\n",
      "    FastICA(tol=0.30000000000000004),\r\n",
      "    ExtraTreesClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.9000000000000001, min_samples_leaf=2, min_samples_split=5, n_estimators=100)\r\n",
      ")\r\n",
      "# Fix random state for all the steps in exported pipeline\r\n",
      "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\r\n",
      "\r\n",
      "exported_pipeline.fit(training_features, training_target)\r\n",
      "results = exported_pipeline.predict(testing_features)\r\n"
     ]
    }
   ],
   "source": [
    "!cat tpot_electronics_pipeline.py # A sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Take the appropriate lines (e.g., updating path to data and the variable names) from `tpot_electronics_pipeline.py` to build a model on our training set and make predictions on the test set. \n",
    "\n",
    "    Choose approriate metrics to evaluate the prediction from TPOT. \n",
    "    \n",
    "    Optional. Add this to Streamlit `models` in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Visualizations with Streamlit and Plotly!\n",
    "Let's visualize our results in a way that makes it easy to compare our models!  \n",
    "[Streamlit](https://streamlit.io/) is a Python package that makes it easy to create bespoke, dynamic and interactive web apps for visualizations.  \n",
    "Lightweight web apps like this are a great way to present results to stakeholders! \n",
    "\n",
    "First we need to build our results dataset to render in Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results_data = defaultdict(list)\n",
    "\n",
    "models = [\"Logistic Regression\", \"SVM\", \"Gradient Boosting Classifier\"]\n",
    "\n",
    "for model, pred in zip(models, [lr_pred, svm_pred, gbt_pred]):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy_ = accuracy(y_test, pred)\n",
    "    recall_ = recall(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    \n",
    "    results_data['model'].append(model)\n",
    "    results_data['tn'].append(confusion[0][0])\n",
    "    results_data['fp'].append(confusion[0][1])\n",
    "    results_data['fn'].append(confusion[1][0])\n",
    "    results_data['tp'].append(confusion[1][1])\n",
    "    results_data['accuracy'].append(accuracy_)\n",
    "    results_data['recall'].append(recall_)\n",
    "    results_data['f1_score'].append(f1)\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our results_df to CSV\n",
    "results_df.to_csv(\"../dat/model_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, we need to built Stremlit app. \n",
    "For today's assignment, we wrote the code for you.\n",
    "However, understand the code in `electronics_purchase_predictions_stremlit.py` because you might need to write a Streamlit web app on your own for the future assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Once you inspected the code, you will need to start streamlit from CLI. \n",
    "Try looking up how to do this yourself.\n",
    "\n",
    "HINT: this is a python file - maybe if you run it as a python file from the CLI, it will give you further instructions. \n",
    "\n",
    "Note - once you run the stremlit, the prompt will ask you to provide an email. \n",
    "You can leave the email blank. \n",
    "\n",
    "And you are done! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This work is motivated by this [publication](https://arxiv.org/pdf/2010.02503.pdf) \n",
    "- [Comprehensive Guide on Feature Selection](https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection/notebook)\n",
    "- [Common pitfalls and recommended practices](https://scikit-learn.org/stable/common_pitfalls.html)\n",
    "- [Interactive Shapley Value Demonstration in Python](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/Interactive_Shapley_Values.ipynb)\n",
    "- [Subsurface Data Analytics](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Feature_Ranking.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week2_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('stock-predictor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f4562350e0bbdbfb77a75b661ad69a21227450e3308323b1b77946ed9dd272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
